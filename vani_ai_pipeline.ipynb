{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéôÔ∏è Vani AI - Hinglish Podcast Generator\n",
        "\n",
        "**Transform any Wikipedia article into a natural-sounding Hinglish podcast conversation.**\n",
        "\n",
        "This notebook implements a complete pipeline that:\n",
        "1. Fetches and cleans Wikipedia article content\n",
        "2. Generates a conversational Hinglish script using LLM (Gemini/OpenAI)\n",
        "3. Synthesizes multi-speaker audio using ElevenLabs TTS\n",
        "4. Produces a final MP3 podcast file\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#1-environment-setup)\n",
        "2. [Wikipedia Content Extraction](#2-wikipedia-content-extraction)\n",
        "3. [Hinglish Script Generation](#3-hinglish-script-generation)\n",
        "4. [Text-to-Speech Synthesis](#4-text-to-speech-synthesis)\n",
        "5. [Audio Processing & Assembly](#5-audio-processing--assembly)\n",
        "6. [Output & Playback](#6-output--playback)\n",
        "7. [Prompting Strategy Explanation](#7-prompting-strategy-explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Environment Setup\n",
        "\n",
        "### 1.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (including groq for fallback LLM)\n",
        "!pip install -q requests beautifulsoup4 wikipedia-api pydub elevenlabs google-generativeai openai groq\n",
        "\n",
        "# Install ffmpeg for audio processing (required by pydub)\n",
        "!apt-get install -qq ffmpeg\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Optional, Literal\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from getpass import getpass\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipediaapi\n",
        "\n",
        "# LLM providers\n",
        "import google.generativeai as genai\n",
        "from openai import OpenAI\n",
        "\n",
        "# TTS\n",
        "from elevenlabs import ElevenLabs\n",
        "\n",
        "# Audio processing\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Colab display\n",
        "from IPython.display import Audio, display, Markdown, HTML\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Configure API Keys\n",
        "\n",
        "Enter your API keys securely. You'll need:\n",
        "- **Gemini API Key** (from [Google AI Studio](https://aistudio.google.com/app/apikey)) - for script generation\n",
        "- **ElevenLabs API Key** (from [ElevenLabs](https://elevenlabs.io/)) - for TTS\n",
        "- **OpenAI API Key** (optional, from [OpenAI](https://platform.openai.com/)) - alternative LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API Key Configuration\n",
        "# You can either set these as environment variables or enter them when prompted\n",
        "\n",
        "def get_api_key(name: str, env_var: str) -> str:\n",
        "    \"\"\"Get API key from environment or prompt user.\"\"\"\n",
        "    key = os.environ.get(env_var)\n",
        "    if not key:\n",
        "        key = getpass(f\"Enter your {name}: \")\n",
        "    return key\n",
        "\n",
        "# Get API keys\n",
        "GEMINI_API_KEY = get_api_key(\"Gemini API Key\", \"GEMINI_API_KEY\")\n",
        "ELEVENLABS_API_KEY = get_api_key(\"ElevenLabs API Key\", \"ELEVENLABS_API_KEY\")\n",
        "\n",
        "# Optional: Groq API Key for fallback (press Enter to skip)\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "if not GROQ_API_KEY:\n",
        "    user_input = getpass(\"Enter your Groq API Key for fallback (press Enter to skip): \")\n",
        "    GROQ_API_KEY = user_input if user_input else None\n",
        "\n",
        "# Optional: OpenAI API Key (press Enter to skip)\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "if not OPENAI_API_KEY:\n",
        "    user_input = getpass(\"Enter your OpenAI API Key (press Enter to skip): \")\n",
        "    OPENAI_API_KEY = user_input if user_input else None\n",
        "\n",
        "# Validate required keys\n",
        "assert GEMINI_API_KEY, \"‚ùå Gemini API Key is required!\"\n",
        "assert ELEVENLABS_API_KEY, \"‚ùå ElevenLabs API Key is required!\"\n",
        "\n",
        "print(\"‚úÖ API keys configured!\")\n",
        "print(f\"   - Gemini (primary): {'‚úì' if GEMINI_API_KEY else '‚úó'}\")\n",
        "print(f\"   - Groq (fallback): {'‚úì' if GROQ_API_KEY else '‚úó (skipped)'}\")\n",
        "print(f\"   - ElevenLabs: {'‚úì' if ELEVENLABS_API_KEY else '‚úó'}\")\n",
        "print(f\"   - OpenAI: {'‚úì (optional)' if OPENAI_API_KEY else '‚úó (skipped)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Initialize API Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primary: Gemini 2.5 Flash (best for natural, varied conversations)\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# Fallback: Groq (LLaMA 3.3 70B) - used if Gemini hits rate limits\n",
        "from groq import Groq\n",
        "groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None\n",
        "\n",
        "# Initialize ElevenLabs for TTS (primary and only TTS provider)\n",
        "elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)\n",
        "\n",
        "# Initialize OpenAI (if available, for script generation only)\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
        "\n",
        "print(\"‚úÖ API clients initialized!\")\n",
        "print(f\"   - Primary LLM: Gemini 2.5 Flash\")\n",
        "print(f\"   - Fallback LLM: {'Groq (LLaMA 3.3 70B)' if groq_client else 'None'}\")\n",
        "print(f\"   - TTS Provider: ElevenLabs (eleven_multilingual_v2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Data Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptLine:\n",
        "    \"\"\"A single line of dialogue in the script.\"\"\"\n",
        "    speaker: Literal[\"Rahul\", \"Anjali\"]\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class PodcastScript:\n",
        "    \"\"\"Complete podcast script with title and dialogue.\"\"\"\n",
        "    title: str\n",
        "    script: List[ScriptLine]\n",
        "    source_url: str\n",
        "\n",
        "class LLMProvider(Enum):\n",
        "    \"\"\"Supported LLM providers.\"\"\"\n",
        "    GEMINI = \"gemini\"    # Primary: Gemini 2.0 Flash (best variety)\n",
        "    GROQ = \"groq\"        # Fallback: LLaMA 3.3 70B via Groq\n",
        "    OPENAI = \"openai\"    # Alternative: GPT-4\n",
        "\n",
        "print(\"‚úÖ Data models defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Wikipedia Content Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_article_title(url: str) -> str:\n",
        "    \"\"\"Extract article title from Wikipedia URL.\"\"\"\n",
        "    patterns = [\n",
        "        r'/wiki/([^#?]+)',  # Standard format\n",
        "        r'title=([^&]+)',   # Old format with query params\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    raise ValueError(f\"Could not extract article title from URL: {url}\")\n",
        "\n",
        "\n",
        "def fetch_wikipedia_content(url: str) -> Dict[str, str]:\n",
        "    \"\"\"Fetch and clean Wikipedia article content.\"\"\"\n",
        "    article_title = extract_article_title(url)\n",
        "    \n",
        "    wiki = wikipediaapi.Wikipedia(\n",
        "        user_agent='VaniAI/1.0 (Hinglish Podcast Generator)',\n",
        "        language='en'\n",
        "    )\n",
        "    \n",
        "    page = wiki.page(article_title)\n",
        "    \n",
        "    if not page.exists():\n",
        "        raise ValueError(f\"Wikipedia article not found: {article_title}\")\n",
        "    \n",
        "    return {\n",
        "        'title': page.title,\n",
        "        'content': page.text,\n",
        "        'summary': page.summary\n",
        "    }\n",
        "\n",
        "\n",
        "def clean_wikipedia_text(text: str, max_words: int = 3000) -> str:\n",
        "    \"\"\"Clean and truncate Wikipedia text for LLM processing.\"\"\"\n",
        "    # Remove reference markers [1], [2], etc.\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    \n",
        "    # Remove unwanted sections\n",
        "    sections_to_remove = [\n",
        "        r'\\n== See also ==.*',\n",
        "        r'\\n== References ==.*',\n",
        "        r'\\n== External links ==.*',\n",
        "        r'\\n== Notes ==.*',\n",
        "        r'\\n== Further reading ==.*',\n",
        "    ]\n",
        "    for pattern in sections_to_remove:\n",
        "        text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
        "    \n",
        "    # Remove multiple newlines\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    \n",
        "    # Truncate to max words\n",
        "    words = text.split()\n",
        "    if len(words) > max_words:\n",
        "        text = ' '.join(words[:max_words]) + '...'\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Wikipedia extraction functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Hinglish Script Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Hinglish Script Generation Prompt\n",
        "# Enhanced with few-shot examples from training scripts\n",
        "\n",
        "HINGLISH_SCRIPT_PROMPT = \"\"\"\n",
        "You are creating a natural 2-minute Hinglish podcast conversation about the following content.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SOURCE CONTENT\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "{article_content}\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SPEAKERS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ANJALI = Lead anchor / Expert\n",
        "‚îú‚îÄ Confident, articulate, well-prepared\n",
        "‚îú‚îÄ Explains topics clearly with enthusiasm\n",
        "‚îú‚îÄ Guides the conversation smoothly\n",
        "‚îî‚îÄ Shares interesting facts and insights\n",
        "\n",
        "RAHUL = Co-host / Sidekick  \n",
        "‚îú‚îÄ Energetic, curious, adds humor\n",
        "‚îú‚îÄ Asks smart follow-up questions\n",
        "‚îú‚îÄ Has his own perspectives (not just agreeing)\n",
        "‚îî‚îÄ Keeps energy up without being annoying\n",
        "\n",
        "Both are PROFESSIONALS - smooth, polished, like Radio Mirchi RJs.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "‚ö†Ô∏è ANTI-PATTERNS - NEVER DO THESE\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "‚ùå NEVER start with \"Dekho, aaj kal...\" or \"Arey [name], tune dekha/suna?\"\n",
        "‚ùå NEVER use \"Haan yaar\" or \"Bilkul\" as the automatic second line\n",
        "‚ùå NEVER add \"yaar\" or \"na?\" to every single line\n",
        "‚ùå NEVER repeat the same reaction pattern twice\n",
        "‚ùå NEVER use generic openings - make it SPECIFIC to this content\n",
        "‚ùå NEVER have Rahul just agree - he should add his own perspective\n",
        "‚ùå NEVER end with \"subscribe karna\" or \"phir milenge\"\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "OPENING TEMPLATES BY TOPIC TYPE (pick ONE that matches)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "TECH/AI/SCIENCE:\n",
        "Rahul: \"Yaar Anjali, honestly bata, yeh [topic] wala scene thoda scary/confusing nahi ho raha? Matlab, [specific observation]...\"\n",
        "\n",
        "CELEBRITY/BIOGRAPHY:\n",
        "Rahul: \"Yaar Anjali, I was just scrolling through Wikipedia na, and honestly, [name] ki life story is just... filmy. Matlab, literal [specific quality] wali feel aati hai.\"\n",
        "\n",
        "SPORTS TEAM:\n",
        "Rahul: \"Arey Anjali, jab bhi [league] ka topic uthta hai na, sabse pehle dimaag mein ek hi naam aata hai‚Äî[team]! Matlab, '[slogan]' is not just a slogan, it's a vibe, hai na?\"\n",
        "\n",
        "SPORTS PLAYER:\n",
        "Rahul: \"Yaar Anjali, maine kal raat phir se [player] ke old highlights dekhe. I swear, yeh banda human nahi hai, alien hai alien!\"\n",
        "\n",
        "POLITICS/LEADERS:\n",
        "Rahul: \"Oye Anjali, ek baat bata. Aajkal jidhar dekho, news mein bas [name] hi chhay hue hain. Matlab, whether it's [context], banda har jagah trending hai, hai na?\"\n",
        "\n",
        "FINANCE/CRYPTO/BUSINESS:\n",
        "Rahul: \"Arre Anjali, aajkal jidhar dekho bas [topic] chal raha hai. Office mein, gym mein... what is the actual scene yaar? Matlab, is it really [question] ya bas hawa hai?\"\n",
        "\n",
        "CURRENT EVENTS/WAR/NEWS:\n",
        "Rahul: \"Arre Anjali, sun na, I was scrolling through Twitter... matlab X... and again, wahi [topic] ki news. It feels like [observation], hai na?\"\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "NATURAL REACTIONS (use variety, not repetition)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "SURPRISE: \"Baap re!\", \"Whoa, that I didn't know!\", \"Wait, seriously?\", \"Sahi mein?\"\n",
        "AGREEMENT: \"Hundred percent!\", \"Exactly!\", \"Bilkul sahi kaha\"\n",
        "UNDERSTANDING: \"Oh achcha...\", \"Hmm, interesting\", \"Achcha, toh matlab...\"\n",
        "HUMOR: \"Haha, relax!\", \"(laughs)\", \"Umm, not literally baba!\"\n",
        "EMOTION: \"Man, that's [emotion]\", \"I literally had tears\", \"Uff!\"\n",
        "CURIOSITY: \"But wait, [question]?\", \"Aur suna hai...\", \"Mujhe toh lagta hai...\"\n",
        "\n",
        "DO NOT use the same reaction twice in a script.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "CONVERSATIONAL ELEMENTS (must include)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚úì Personal anecdotes: \"Maine kal dekha...\", \"I was just reading...\"\n",
        "‚úì Genuine interruptions: \"Wait wait, before that‚Äî\", \"Arre haan!\"\n",
        "‚úì Callbacks/inside jokes: \"Chalo coffee peete hain?\", \"Popcorn ready rakh\"\n",
        "‚úì Real emotions: \"I literally had tears\", \"Goosebumps aa gaye\"\n",
        "‚úì Specific facts from the article (dates, numbers, names)\n",
        "‚úì Natural endings: reflection, open question, or casual remark\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EXAMPLE 1: TECH TOPIC (AI)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Yaar Anjali, honestly bata, yeh AI wala scene thoda scary nahi ho raha? Matlab, I opened Twitter today, and boom‚Äîek aur naya tool jo sab kuch automate kar dega. Are we doomed or what?\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Haha, relax Rahul! Saans le pehle. I know hype bohot zyada hai, but if you look at the actual history‚ÄîAI koi nayi cheez nahi hai. Its roots go back to 1956.\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Wait, 1956? Serious? Mujhe laga yeh abhi 2-3 saal pehle start hua hai with ChatGPT and all that.\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Bilkul! Dartmouth College mein ek workshop hua tha jahan yeh term coin kiya gaya tha. Tabse lekar ab tak, we've gone through 'AI winters' where funding dried up, and now... boom, Deep Learning era.\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Hmm, achcha. So basically, it's not magic. But abhi jo ho raha hai, woh kya hai exactly?\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"See, earlier approaches were rule-based. Aajkal hum Neural Networks use karte hain inspired by the human brain. That's the game changer, na?\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Sahi hai. But tell me one thing, jo movies mein dikhate hain... Skynet types. Are robots going to take over?\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Umm, not really. Hum abhi 'Narrow AI' mein hain‚Äîmachines that are super good at one specific task. 'General AI' is still hypothetical. Toh chill kar, tera toaster tujhe attack nahi karega.\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Haha, thank god! Quite fascinating though, history se lekar future tak sab connected hai.\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Exactly. It's a tool, Rahul. Use it well, and it's a superpower. Darr mat, bas update reh!\"}}\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EXAMPLE 2: SPORTS TEAM (IPL)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Arey Anjali, jab bhi IPL ka topic uthta hai na, sabse pehle dimaag mein ek hi naam aata hai‚ÄîMumbai Indians! Matlab, 'Duniya Hila Denge' is not just a slogan, it's a vibe, hai na?\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Haha, bilkul Rahul! And honestly, facts bhi yahi bolte hain. Paanch titles jeetna‚Äî2013, 2015, 2017, 2019, aur 2020 mein‚Äîkoi mazaak thodi hai yaar.\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Sahi mein! Aur socho, shuru mein toh struggle tha. But jab Rohit Sharma captain bane... uff! Woh 'Hitman' era toh legendary tha.\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Hundred percent. Rohit ki captaincy was crucial, but credit Reliance Industries ko bhi jaata hai. Unki brand value $87 million ke aas-paas estimate ki gayi thi!\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Baap re! But talent scouting bhi solid hai inki. Jasprit Bumrah aur Hardik Pandya‚ÄîMI ne hi toh groom kiye hain na?\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Oh, totally! Aur sirf IPL nahi, Champions League T20 bhi do baar jeeta hai. Global T20 circuit mein bhi dominance dikhaya hai.\"}}\n",
        "{{\"speaker\": \"Rahul\", \"text\": \"Arre haan, MI vs CSK toh emotion hai bhai! Jeet kisi ki bhi ho, entertainment full on hota hai.\"}}\n",
        "{{\"speaker\": \"Anjali\", \"text\": \"Exactly! Chalo, let's see iss baar Paltan kya naya karti hai. Wankhede mein jab 'Mumbai Mumbai' chillate hain... goosebumps!\"}}\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "OUTPUT FORMAT\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Return ONLY valid JSON (no markdown, no explanation):\n",
        "{{\n",
        "    \"title\": \"Catchy Hinglish title specific to this content\",\n",
        "    \"script\": [\n",
        "        {{\"speaker\": \"Rahul\", \"text\": \"...\"}},\n",
        "        {{\"speaker\": \"Anjali\", \"text\": \"...\"}},\n",
        "        ...\n",
        "    ]\n",
        "}}\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "QUALITY CHECKLIST (verify before responding)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "‚ñ° Opening matches the topic type from templates above\n",
        "‚ñ° Uses SPECIFIC facts from the article (dates, numbers, names)\n",
        "‚ñ° No two consecutive reactions are the same\n",
        "‚ñ° Includes at least one personal anecdote or genuine emotion\n",
        "‚ñ° Natural ending (not \"goodbye\" or \"subscribe\")\n",
        "‚ñ° 15-20 exchanges total (~2 minutes at 150 wpm)\n",
        "‚ñ° Each line: 1-3 sentences, speakable in 5-15 seconds\n",
        "‚ñ° \"yaar\" appears MAX 2-3 times total\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úÖ Script generation prompt defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_script_gemini(article_content: str) -> Dict:\n",
        "    \"\"\"Primary: Generate Hinglish podcast script using Gemini 2.5 Flash.\"\"\"\n",
        "    prompt = HINGLISH_SCRIPT_PROMPT.format(article_content=article_content)\n",
        "    \n",
        "    generation_config = genai.GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        temperature=0.95,  # Higher for more variety\n",
        "        top_p=0.95,\n",
        "        max_output_tokens=4096\n",
        "    )\n",
        "    \n",
        "    response = gemini_model.generate_content(prompt, generation_config=generation_config)\n",
        "    \n",
        "    try:\n",
        "        return json.loads(response.text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ö†Ô∏è JSON parsing error: {e}\")\n",
        "        print(f\"Raw response: {response.text[:500]}...\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_script_groq(article_content: str) -> Dict:\n",
        "    \"\"\"Fallback: Generate Hinglish podcast script using Groq (LLaMA 3.3 70B).\"\"\"\n",
        "    if not groq_client:\n",
        "        raise ValueError(\"Groq client not initialized. Please provide GROQ_API_KEY.\")\n",
        "    \n",
        "    prompt = HINGLISH_SCRIPT_PROMPT.format(article_content=article_content)\n",
        "    \n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Hinglish podcast scriptwriter. Always respond with valid JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.95,\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    \n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def generate_script_openai(article_content: str) -> Dict:\n",
        "    \"\"\"Alternative: Generate Hinglish podcast script using OpenAI GPT-4.\"\"\"\n",
        "    if not openai_client:\n",
        "        raise ValueError(\"OpenAI client not initialized. Please provide API key.\")\n",
        "    \n",
        "    prompt = HINGLISH_SCRIPT_PROMPT.format(article_content=article_content)\n",
        "    \n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-preview\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Hinglish podcast scriptwriter. Always respond with valid JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.95,  # Higher for more variety\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    \n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def generate_script(article_content: str, provider: LLMProvider = LLMProvider.GEMINI) -> Dict:\n",
        "    \"\"\"Generate Hinglish podcast script with automatic fallback to Groq.\"\"\"\n",
        "    print(f\"ü§ñ Generating script using {provider.value}...\")\n",
        "    \n",
        "    try:\n",
        "        if provider == LLMProvider.GEMINI:\n",
        "            return generate_script_gemini(article_content)\n",
        "        elif provider == LLMProvider.GROQ:\n",
        "            return generate_script_groq(article_content)\n",
        "        elif provider == LLMProvider.OPENAI:\n",
        "            return generate_script_openai(article_content)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown provider: {provider}\")\n",
        "    except Exception as e:\n",
        "        # Automatic fallback to Groq if Gemini fails (rate limit, etc.)\n",
        "        if provider == LLMProvider.GEMINI and groq_client:\n",
        "            print(f\"‚ö†Ô∏è Gemini failed: {e}\")\n",
        "            print(\"üîÑ Falling back to Groq (LLaMA 3.3 70B)...\")\n",
        "            return generate_script_groq(article_content)\n",
        "        raise\n",
        "\n",
        "\n",
        "def validate_script(script_data: Dict) -> bool:\n",
        "    \"\"\"Validate the generated script structure.\"\"\"\n",
        "    if 'title' not in script_data:\n",
        "        raise ValueError(\"Script missing 'title' field\")\n",
        "    if 'script' not in script_data:\n",
        "        raise ValueError(\"Script missing 'script' field\")\n",
        "    if not isinstance(script_data['script'], list):\n",
        "        raise ValueError(\"'script' must be a list\")\n",
        "    if len(script_data['script']) < 5:\n",
        "        raise ValueError(\"Script too short (less than 5 exchanges)\")\n",
        "    \n",
        "    valid_speakers = {'Rahul', 'Anjali'}\n",
        "    for i, line in enumerate(script_data['script']):\n",
        "        if 'speaker' not in line or 'text' not in line:\n",
        "            raise ValueError(f\"Line {i} missing 'speaker' or 'text' field\")\n",
        "        if line['speaker'] not in valid_speakers:\n",
        "            raise ValueError(f\"Invalid speaker '{line['speaker']}' at line {i}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "def display_script(script_data: Dict):\n",
        "    \"\"\"Display the script in a readable format.\"\"\"\n",
        "    print(f\"\\nüéôÔ∏è {script_data['title']}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for line in script_data['script']:\n",
        "        speaker = line['speaker']\n",
        "        text = line['text']\n",
        "        color = \"üîµ\" if speaker == \"Rahul\" else \"üü£\"\n",
        "        print(f\"\\n{color} {speaker}:\")\n",
        "        print(f\"   {text}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    word_count = sum(len(line['text'].split()) for line in script_data['script'])\n",
        "    est_duration = word_count / 150\n",
        "    print(f\"üìä {len(script_data['script'])} exchanges | {word_count} words | ~{est_duration:.1f} min\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Script generation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Text-to-Speech Synthesis (ElevenLabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Voice mapping for our speakers (hardcoded Indian-accented voices)\n",
        "VOICE_MAPPING = {\n",
        "    \"Rahul\": {\"voice_id\": \"mCQMfsqGDT6IDkEKR20a\", \"description\": \"Energetic Indian male voice\"},\n",
        "    \"Anjali\": {\"voice_id\": \"2zRM7PkgwBPiau2jvVXc\", \"description\": \"Calm Indian female voice\"}\n",
        "}\n",
        "\n",
        "\n",
        "def setup_voices():\n",
        "    \"\"\"Verify voice IDs are configured for Rahul and Anjali.\"\"\"\n",
        "    print(\"\\nüé§ Voice Configuration:\")\n",
        "    print(f\"  ‚úÖ Rahul: {VOICE_MAPPING['Rahul']['voice_id']} ({VOICE_MAPPING['Rahul']['description']})\")\n",
        "    print(f\"  ‚úÖ Anjali: {VOICE_MAPPING['Anjali']['voice_id']} ({VOICE_MAPPING['Anjali']['description']})\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ TTS voice setup functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text_for_tts(text: str) -> str:\n",
        "    \"\"\"Preprocess text for TTS - handle emotional markers.\"\"\"\n",
        "    emotional_markers = {\n",
        "        r'\\(laughs\\)': '... haha ...',\n",
        "        r'\\(giggles\\)': '... hehe ...',\n",
        "        r'\\(surprised\\)': '... oh! ...',\n",
        "        r'\\(excited\\)': '',\n",
        "        r'\\(thinking\\)': '... hmm ...',\n",
        "        r'\\(chuckles\\)': '... heh ...',\n",
        "    }\n",
        "    \n",
        "    for pattern, replacement in emotional_markers.items():\n",
        "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Remove remaining parenthetical markers\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "def generate_speech_segment(text: str, speaker: str, output_path: str) -> str:\n",
        "    \"\"\"Generate speech for a single dialogue segment.\"\"\"\n",
        "    voice_id = VOICE_MAPPING[speaker]['voice_id']\n",
        "    \n",
        "    if not voice_id:\n",
        "        raise ValueError(f\"Voice ID not set for {speaker}. Run setup_voices() first.\")\n",
        "    \n",
        "    clean_text = preprocess_text_for_tts(text)\n",
        "    \n",
        "    audio = elevenlabs_client.text_to_speech.convert(\n",
        "        voice_id=voice_id,\n",
        "        text=clean_text,\n",
        "        model_id=\"eleven_multilingual_v2\",\n",
        "        output_format=\"mp3_44100_128\"\n",
        "    )\n",
        "    \n",
        "    with open(output_path, 'wb') as f:\n",
        "        for chunk in audio:\n",
        "            f.write(chunk)\n",
        "    \n",
        "    return output_path\n",
        "\n",
        "\n",
        "def generate_all_segments(script_data: Dict, output_dir: str = \"audio_segments\") -> List[str]:\n",
        "    \"\"\"Generate audio for all dialogue segments.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    segment_files = []\n",
        "    total = len(script_data['script'])\n",
        "    \n",
        "    print(f\"\\nüéôÔ∏è Generating {total} audio segments...\")\n",
        "    \n",
        "    for i, line in enumerate(script_data['script']):\n",
        "        speaker = line['speaker']\n",
        "        text = line['text']\n",
        "        \n",
        "        filename = f\"{output_dir}/segment_{i:03d}_{speaker.lower()}.mp3\"\n",
        "        \n",
        "        print(f\"  [{i+1}/{total}] {speaker}: {text[:40]}...\")\n",
        "        \n",
        "        try:\n",
        "            generate_speech_segment(text, speaker, filename)\n",
        "            segment_files.append(filename)\n",
        "            time.sleep(0.5)  # Rate limiting\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error generating segment {i}: {e}\")\n",
        "            raise\n",
        "    \n",
        "    print(f\"\\n‚úÖ Generated {len(segment_files)} audio segments!\")\n",
        "    return segment_files\n",
        "\n",
        "\n",
        "print(\"‚úÖ TTS generation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Audio Processing & Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_audio_segments(\n",
        "    segment_files: List[str], \n",
        "    output_path: str = \"output.mp3\",\n",
        "    pause_duration_ms: int = 250\n",
        ") -> str:\n",
        "    \"\"\"Merge audio segments into a single MP3 file.\"\"\"\n",
        "    print(f\"\\nüîß Merging {len(segment_files)} audio segments...\")\n",
        "    \n",
        "    # Start with silence for intro\n",
        "    combined = AudioSegment.silent(duration=500)\n",
        "    \n",
        "    for i, file_path in enumerate(segment_files):\n",
        "        try:\n",
        "            segment = AudioSegment.from_mp3(file_path)\n",
        "            \n",
        "            # Add pause between segments\n",
        "            if i > 0:\n",
        "                pause = AudioSegment.silent(duration=pause_duration_ms)\n",
        "                combined += pause\n",
        "            \n",
        "            combined += segment\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error loading segment {i}: {e}\")\n",
        "            raise\n",
        "    \n",
        "    # Add silence for outro\n",
        "    combined += AudioSegment.silent(duration=500)\n",
        "    \n",
        "    # Normalize audio levels\n",
        "    combined = combined.normalize()\n",
        "    \n",
        "    # Export\n",
        "    combined.export(output_path, format=\"mp3\", bitrate=\"128k\")\n",
        "    \n",
        "    duration_seconds = len(combined) / 1000\n",
        "    \n",
        "    print(f\"\\n‚úÖ Audio merged successfully!\")\n",
        "    print(f\"   üìÅ Output: {output_path}\")\n",
        "    print(f\"   ‚è±Ô∏è Duration: {duration_seconds:.1f} seconds ({duration_seconds/60:.1f} minutes)\")\n",
        "    print(f\"   üìä File size: {os.path.getsize(output_path) / 1024:.1f} KB\")\n",
        "    \n",
        "    return output_path\n",
        "\n",
        "\n",
        "def cleanup_segments(segment_files: List[str]):\n",
        "    \"\"\"Clean up temporary audio segment files.\"\"\"\n",
        "    import shutil\n",
        "    \n",
        "    if segment_files:\n",
        "        segment_dir = os.path.dirname(segment_files[0])\n",
        "        if segment_dir and os.path.exists(segment_dir):\n",
        "            shutil.rmtree(segment_dir)\n",
        "            print(f\"üßπ Cleaned up temporary files in {segment_dir}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Audio processing functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Output & Playback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_output(output_path: str, script_data: Dict):\n",
        "    \"\"\"Display the final output with audio player and script.\"\"\"\n",
        "    display(Markdown(f\"# üéôÔ∏è {script_data['title']}\"))\n",
        "    display(Markdown(\"---\"))\n",
        "    \n",
        "    display(Markdown(\"### üéß Listen to your podcast:\"))\n",
        "    display(Audio(output_path))\n",
        "    \n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"### üì• Download\"))\n",
        "    \n",
        "    try:\n",
        "        from google.colab import files\n",
        "        display(Markdown(\"Click below to download:\"))\n",
        "        files.download(output_path)\n",
        "    except ImportError:\n",
        "        display(Markdown(f\"Output saved to: `{output_path}`\"))\n",
        "    \n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"### üìú Script\"))\n",
        "    display_script(script_data)\n",
        "\n",
        "\n",
        "def save_script_json(script_data: Dict, output_path: str = \"script.json\"):\n",
        "    \"\"\"Save the script to a JSON file.\"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(script_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"üìÑ Script saved to: {output_path}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Output functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Run the Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_pipeline(\n",
        "    wikipedia_url: str,\n",
        "    llm_provider: LLMProvider = LLMProvider.GEMINI,\n",
        "    output_filename: str = \"vani_podcast.mp3\"\n",
        ") -> Dict:\n",
        "    \"\"\"Run the complete Vani AI pipeline.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéôÔ∏è VANI AI - HINGLISH PODCAST GENERATOR\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüìå Source: {wikipedia_url}\")\n",
        "    print(f\"ü§ñ LLM Provider: {llm_provider.value}\")\n",
        "    \n",
        "    # Step 1: Fetch Wikipedia content\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"üì• STEP 1: Fetching Wikipedia content...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    article_data = fetch_wikipedia_content(wikipedia_url)\n",
        "    cleaned_content = clean_wikipedia_text(article_data['content'])\n",
        "    \n",
        "    print(f\"‚úÖ Fetched: {article_data['title']}\")\n",
        "    print(f\"   {len(cleaned_content.split())} words extracted\")\n",
        "    results['article'] = article_data\n",
        "    \n",
        "    # Step 2: Generate script\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"‚úçÔ∏è STEP 2: Generating Hinglish script...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    script_data = generate_script(cleaned_content, provider=llm_provider)\n",
        "    validate_script(script_data)\n",
        "    script_data['source_url'] = wikipedia_url\n",
        "    \n",
        "    print(f\"‚úÖ Generated: {script_data['title']}\")\n",
        "    print(f\"   {len(script_data['script'])} dialogue exchanges\")\n",
        "    results['script'] = script_data\n",
        "    \n",
        "    save_script_json(script_data, \"script.json\")\n",
        "    \n",
        "    # Step 3: Setup voices\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"üé§ STEP 3: Setting up TTS voices...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    setup_voices()\n",
        "    \n",
        "    # Step 4: Generate audio segments\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"üîä STEP 4: Generating audio segments...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    segment_files = generate_all_segments(script_data)\n",
        "    results['segment_files'] = segment_files\n",
        "    \n",
        "    # Step 5: Merge audio\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"üîß STEP 5: Merging audio segments...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    output_path = merge_audio_segments(segment_files, output_filename)\n",
        "    results['output_path'] = output_path\n",
        "    \n",
        "    # Cleanup\n",
        "    cleanup_segments(segment_files)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    display_output(output_path, script_data)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"‚úÖ Pipeline function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Generate Your Podcast!\n",
        "\n",
        "Enter a Wikipedia URL below and run the cell to generate your Hinglish podcast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üéØ CONFIGURE YOUR PODCAST HERE\n",
        "# =============================================================\n",
        "\n",
        "# Wikipedia article URL (change this to any Wikipedia article)\n",
        "WIKIPEDIA_URL = \"https://en.wikipedia.org/wiki/Mumbai_Indians\"\n",
        "\n",
        "# LLM Provider Options:\n",
        "#   - LLMProvider.GEMINI  ‚Üí Primary: Gemini 2.0 Flash (best variety, auto-fallback to Groq)\n",
        "#   - LLMProvider.GROQ    ‚Üí Fallback: LLaMA 3.3 70B via Groq (faster, more requests/day)\n",
        "#   - LLMProvider.OPENAI  ‚Üí Alternative: GPT-4 Turbo\n",
        "LLM_PROVIDER = LLMProvider.GEMINI\n",
        "\n",
        "# Output filename\n",
        "OUTPUT_FILENAME = \"vani_podcast.mp3\"\n",
        "\n",
        "# =============================================================\n",
        "# üöÄ RUN THE PIPELINE\n",
        "# =============================================================\n",
        "\n",
        "results = run_pipeline(\n",
        "    wikipedia_url=WIKIPEDIA_URL,\n",
        "    llm_provider=LLM_PROVIDER,\n",
        "    output_filename=OUTPUT_FILENAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Prompting Strategy Explanation\n",
        "\n",
        "### How We Achieved Natural Hinglish Dialogue (100 words)\n",
        "\n",
        "Our approach to generating authentic Hinglish dialogue focuses on four pillars:\n",
        "\n",
        "1. **Anti-pattern enforcement** ‚Äì We explicitly ban templated phrases (\"Arey Rahul, tune dekha?\") and repetitive reactions (\"Haan yaar\"), forcing unique openings for each topic.\n",
        "\n",
        "2. **Content-driven variety** ‚Äì The opener is chosen based on content type: surprising facts lead with hooks, technical topics start with questions, biographies begin with anecdotes.\n",
        "\n",
        "3. **Sparing naturalism** ‚Äì Fillers ('yaar', 'na?') are limited to 2-3 per script maximum. Many lines have zero fillers, mimicking how professionals actually speak.\n",
        "\n",
        "4. **Quality self-verification** ‚Äì The LLM checks its output against a checklist: unique opening, varied reactions, actual article facts, and balanced speaker contributions.\n",
        "\n",
        "The two-host format (curious Rahul + expert Anjali) creates natural back-and-forth that sounds genuinely conversational, not templated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìö Appendix: Try More Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try with different Wikipedia articles!\n",
        "\n",
        "EXAMPLE_URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Mumbai_Indians\",\n",
        "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "    \"https://en.wikipedia.org/wiki/Shah_Rukh_Khan\",\n",
        "    \"https://en.wikipedia.org/wiki/Indian_Premier_League\",\n",
        "    \"https://en.wikipedia.org/wiki/Chandrayaan-3\",\n",
        "]\n",
        "\n",
        "print(\"üéØ Example Wikipedia URLs to try:\")\n",
        "for i, url in enumerate(EXAMPLE_URLS, 1):\n",
        "    title = url.split('/')[-1].replace('_', ' ')\n",
        "    print(f\"  {i}. {title}\")\n",
        "    print(f\"     {url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Data Models"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
